{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6202dc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/lts/1.8/torch_lts.html\n",
      "Collecting torch==1.8.2+cu111\n",
      "  Using cached https://download.pytorch.org/whl/lts/1.8/cu111/torch-1.8.2%2Bcu111-cp38-cp38-win_amd64.whl (3057.4 MB)\n",
      "Collecting torchvision==0.9.2+cu111\n",
      "  Downloading https://download.pytorch.org/whl/lts/1.8/cu111/torchvision-0.9.2%2Bcu111-cp38-cp38-win_amd64.whl (1.9 MB)\n",
      "Collecting torchaudio===0.8.2\n",
      "  Downloading https://download.pytorch.org/whl/lts/1.8/torchaudio-0.8.2-cp38-none-win_amd64.whl (109 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from torch==1.8.2+cu111) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from torch==1.8.2+cu111) (1.20.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from torchvision==0.9.2+cu111) (8.2.0)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "Successfully installed torch-1.8.2+cu111 torchaudio-0.8.2 torchvision-0.9.2+cu111\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.8.2+cu111 torchvision==0.9.2+cu111 torchaudio===0.8.2 -f https://download.pytorch.org/whl/lts/1.8/torch_lts.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05ed9d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/PrithivirajDamodaran/Gramformer.git\n",
      "  Cloning https://github.com/PrithivirajDamodaran/Gramformer.git to c:\\users\\akash kundu\\appdata\\local\\temp\\pip-req-build-1unzswwn\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.96-cp38-cp38-win_amd64.whl (1.1 MB)\n",
      "Collecting python-Levenshtein\n",
      "  Using cached python-Levenshtein-0.12.2.tar.gz (50 kB)\n",
      "Collecting fuzzywuzzy\n",
      "  Using cached fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from gramformer==1.0) (0.11.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from gramformer==1.0) (0.9.0)\n",
      "Collecting errant\n",
      "  Using cached errant-2.3.0-py3-none-any.whl (499 kB)\n",
      "Requirement already satisfied: spacy<3,>=2.2.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from errant->gramformer==1.0) (2.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from python-Levenshtein->gramformer==1.0) (52.0.0.post20210125)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.4.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (3.0.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (4.59.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.0.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (1.20.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from spacy<3,>=2.2.0->errant->gramformer==1.0) (7.4.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3,>=2.2.0->errant->gramformer==1.0) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3,>=2.2.0->errant->gramformer==1.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3,>=2.2.0->errant->gramformer==1.0) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3,>=2.2.0->errant->gramformer==1.0) (2.10)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (20.9)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (2021.4.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (5.4.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone -q https://github.com/PrithivirajDamodaran/Gramformer.git 'C:\\Users\\Akash Kundu\\AppData\\Local\\Temp\\pip-req-build-1unzswwn'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: filelock in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers->gramformer==1.0) (3.0.12)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers->gramformer==1.0) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers->gramformer==1.0) (2.4.7)\n",
      "Requirement already satisfied: six in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from sacremoses->transformers->gramformer==1.0) (1.15.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from sacremoses->transformers->gramformer==1.0) (1.0.1)\n",
      "Requirement already satisfied: click in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from sacremoses->transformers->gramformer==1.0) (7.1.2)\n",
      "Building wheels for collected packages: gramformer, python-Levenshtein\n",
      "  Building wheel for gramformer (setup.py): started\n",
      "  Building wheel for gramformer (setup.py): finished with status 'done'\n",
      "  Created wheel for gramformer: filename=gramformer-1.0-py3-none-any.whl size=4449 sha256=5fe0fa6e6ea2cd3df29b9e4e62f3089eb4e48a703bcabbd4c65968dabc375b0e\n",
      "  Stored in directory: C:\\Users\\Akash Kundu\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-qbcnbesc\\wheels\\fc\\6b\\60\\eaf839540721c457b49785aa12744772957896aed0dba01189\n",
      "  Building wheel for python-Levenshtein (setup.py): started\n",
      "  Building wheel for python-Levenshtein (setup.py): finished with status 'done'\n",
      "  Created wheel for python-Levenshtein: filename=python_Levenshtein-0.12.2-cp38-cp38-win_amd64.whl size=83782 sha256=9d3be03e45fb53ee8951308a6ff16c6f91e7c5691beb95553e45ef2520cb8f46\n",
      "  Stored in directory: c:\\users\\akash kundu\\appdata\\local\\pip\\cache\\wheels\\d7\\0c\\76\\042b46eb0df65c3ccd0338f791210c55ab79d209bcc269e2c7\n",
      "Successfully built gramformer python-Levenshtein\n",
      "Installing collected packages: python-Levenshtein, huggingface-hub, transformers, sentencepiece, fuzzywuzzy, errant, gramformer\n",
      "Successfully installed errant-2.3.0 fuzzywuzzy-0.18.0 gramformer-1.0 huggingface-hub-0.4.0 python-Levenshtein-0.12.2 sentencepiece-0.1.96 transformers-4.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U git+https://github.com/PrithivirajDamodaran/Gramformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20e1e2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gramformer import Gramformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef95878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eaa5427",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-382f2986ab0b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mGramformer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gramformer\\gramformer.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, models, use_gpu)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#from lm_scorer.models.auto import AutoLMScorer as LMScorer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0merrant\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mannotator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merrant\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0muse_gpu\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\errant\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(lang, nlp)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Load spacy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ner\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Load language edit merger\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, **overrides)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exists\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Path or Path-like to model data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "gf=Gramformer(models=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ac6f9e7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting happytransformer"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out. (read timeout=15)\")': /packages/a4/45/dcaa113699791da09107c231858ae75f34516064fc4f0db295df3df23b9e/fsspec-2022.3.0-py3-none-any.whl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached happytransformer-2.4.1-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: transformers>=4.4.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from happytransformer) (4.17.0)\n",
      "Requirement already satisfied: tqdm>=4.43 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from happytransformer) (4.59.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from happytransformer) (0.1.96)\n",
      "Collecting datasets>=1.6.0\n",
      "  Using cached datasets-2.0.0-py3-none-any.whl (325 kB)\n",
      "Requirement already satisfied: protobuf in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from happytransformer) (3.19.1)\n",
      "Requirement already satisfied: torch>=1.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from happytransformer) (1.8.2+cu111)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp38-cp38-win_amd64.whl (29 kB)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp38-cp38-win_amd64.whl (555 kB)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from datasets>=1.6.0->happytransformer) (0.4.0)\n",
      "Collecting responses<0.19\n",
      "  Using cached responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting dill\n",
      "  Using cached dill-0.3.4-py2.py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: pyarrow>=5.0.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from datasets>=1.6.0->happytransformer) (6.0.1)\n",
      "Collecting tqdm>=4.43\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from datasets>=1.6.0->happytransformer) (1.20.1)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.12.2-py38-none-any.whl (128 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from datasets>=1.6.0->happytransformer) (2.25.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from datasets>=1.6.0->happytransformer) (1.2.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from datasets>=1.6.0->happytransformer) (20.9)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.6.0->happytransformer) (3.7.4.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.6.0->happytransformer) (3.0.12)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets>=1.6.0->happytransformer) (5.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from packaging->datasets>=1.6.0->happytransformer) (2.4.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from requests>=2.19.0->datasets>=1.6.0->happytransformer) (1.26.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from tqdm>=4.43->happytransformer) (0.4.4)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers>=4.4.0->happytransformer) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers>=4.4.0->happytransformer) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from transformers>=4.4.0->happytransformer) (0.11.6)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp38-cp38-win_amd64.whl (122 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from aiohttp->datasets>=1.6.0->happytransformer) (20.3.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp38-cp38-win_amd64.whl (33 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp38-cp38-win_amd64.whl (28 kB)\n",
      "Collecting charset-normalizer<3.0,>=2.0\n",
      "  Downloading charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from pandas->datasets>=1.6.0->happytransformer) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from pandas->datasets>=1.6.0->happytransformer) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas->datasets>=1.6.0->happytransformer) (1.15.0)\n",
      "Requirement already satisfied: click in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=4.4.0->happytransformer) (7.1.2)\n",
      "Requirement already satisfied: joblib in c:\\users\\akash kundu\\anaconda3\\lib\\site-packages (from sacremoses->transformers>=4.4.0->happytransformer) (1.0.1)\n",
      "Installing collected packages: multidict, frozenlist, yarl, charset-normalizer, async-timeout, aiosignal, tqdm, fsspec, dill, aiohttp, xxhash, responses, multiprocess, datasets, happytransformer\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.59.0\n",
      "    Uninstalling tqdm-4.59.0:\n",
      "      Successfully uninstalled tqdm-4.59.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.9.0\n",
      "    Uninstalling fsspec-0.9.0:\n",
      "      Successfully uninstalled fsspec-0.9.0\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 charset-normalizer-2.0.12 datasets-2.0.0 dill-0.3.4 frozenlist-1.3.0 fsspec-2022.3.0 happytransformer-2.4.1 multidict-6.0.2 multiprocess-0.70.12.2 responses-0.18.0 tqdm-4.64.0 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install happytransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10c626e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708dbea70bb7482999a8416a90bc68bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c97a272cd5c4f11b087646f41bad23d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5b9949834514e4590aba9ed5eef75ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eef1df8f83b4262898f499bd3ce3c45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2022 14:51:14 - INFO - happytransformer.happy_transformer -   Using model: cpu\n"
     ]
    }
   ],
   "source": [
    "from happytransformer import HappyTextToText\n",
    "\n",
    "happy_tt = HappyTextToText(\"T5\", \"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87ee730e",
   "metadata": {},
   "outputs": [],
   "source": [
    " from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45cb9332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a4ad3e4940e49708ab39662be197714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86f9276bcd44411ca784d4d536861469",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.71k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset jfleg/default (download: 713.98 KiB, generated: 741.90 KiB, post-processed: Unknown size, total: 1.42 MiB) to C:\\Users\\Akash Kundu\\.cache\\huggingface\\datasets\\jfleg\\default\\1.0.0\\ed4ab2367351fe31949f48849ae6732b164f0d5ea6bb5d4357ff4293ac89511b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "176d1fbb41a1437d887d675eb567c40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f5240e44d3414aafe42e89786a17a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/27.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd1ac18fd95452eae65610496970cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cfda7311cf45a185b3e5f7e8cb073f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035bcc0000564a18b720247cc41ede5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd2e53edb6c44b58f6c08218a7237c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9816bbfdc1b247c7a74db7256733948b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abafd7e5f34e407ab66f17108466875b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8ddb3e758542a89f575c959aa3148b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/27.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf76cf25a06f498dbe50e8cf2260d12f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16cc550c6a6f43b0aed80185197c8e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffafaad064864d63a88af4632248e4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a357c5f8c9eb44c9b6788520a88b6ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/26.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8077b93a3fed452690fdeb24764f05a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/755 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/748 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset jfleg downloaded and prepared to C:\\Users\\Akash Kundu\\.cache\\huggingface\\datasets\\jfleg\\default\\1.0.0\\ed4ab2367351fe31949f48849ae6732b164f0d5ea6bb5d4357ff4293ac89511b. Subsequent calls will reuse this data.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = load_dataset(\"jfleg\", split='validation[:]')\n",
    "\n",
    "eval_dataset = load_dataset(\"jfleg\", split='test[:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b61f1e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'corrections'],\n",
       "    num_rows: 755\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10cbfcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['So I think we would not be alive if our ancestors did not develop sciences and technologies . ', 'So I think we could not live if older people did not develop science and technologies . ', 'So I think we can not live if old people could not find science and technologies and they did not develop . ', 'So I think we can not live if old people can not find the science and technology that has not been developed . ']\n",
      "So I think we would not be alive if our ancestors did not develop sciences and technologies . \n",
      "--------------------------------------------------------\n",
      "['Not for use with a car . ', 'Do not use in the car . ', 'Car not for use . ', 'Can not use the car . ']\n",
      "Not for use with a car . \n",
      "--------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for case in train_dataset[\"corrections\"][:2]:\n",
    "  print(case)\n",
    "  print(case[0])\n",
    "  print(\"--------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "961a37ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def generate_csv(csv_path, dataset):\n",
    "    with open(csv_path, 'w', newline='') as csvfile:\n",
    "        writter = csv.writer(csvfile)\n",
    "        writter.writerow([\"input\", \"target\"])\n",
    "        for case in dataset:\n",
    "     \t    # Adding the task's prefix to input \n",
    "            input_text = \"grammar: \" + case[\"sentence\"]\n",
    "            for correction in case[\"corrections\"]:\n",
    "                # a few of the cases contain blank strings. \n",
    "                if input_text and correction:\n",
    "                    writter.writerow([input_text, correction])\n",
    "                    \n",
    "\n",
    "\n",
    "generate_csv(\"train.csv\", train_dataset)\n",
    "generate_csv(\"eval.csv\", eval_dataset)\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "304dda36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2022 15:41:49 - INFO - happytransformer.happy_transformer -   Preprocessing evaluating data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Akash Kundu\\.cache\\huggingface\\datasets\\csv\\default-68d9d9501ed1a10f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2631e5edd44c6d91a9d65d4cdfb666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e80b658852495193d35e9929e642bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Akash Kundu\\.cache\\huggingface\\datasets\\csv\\default-68d9d9501ed1a10f\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffcaee1d97834c58bb2c6dd2048abdab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e566d683ca64fbf9c3673e8f5240404",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2988\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2988' max='2988' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2988/2988 13:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    " before_result = happy_tt.eval(\"eval.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "696cb732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before loss: 1.28038489818573\n"
     ]
    }
   ],
   "source": [
    "print(\"Before loss:\", before_result.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47c0404",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2022 15:56:40 - INFO - happytransformer.happy_transformer -   Preprocessing training data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to C:\\Users\\Akash Kundu\\.cache\\huggingface\\datasets\\csv\\default-46cd6c9a7e45c0fd\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e555f319510248d7af29bf9d4554ba86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b68365183dde43cea9a92d9196472894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:\\Users\\Akash Kundu\\.cache\\huggingface\\datasets\\csv\\default-46cd6c9a7e45c0fd\\0.0.0\\433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bf4e8e889464e3db96b754dd845b32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01fca812744645d98f0f499c4966d033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/04/2022 15:56:49 - INFO - happytransformer.happy_transformer -   Training...\n",
      "PyTorch: setting up devices\n",
      "C:\\Users\\Akash Kundu\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3016\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1131\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='207' max='1131' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 207/1131 42:42 < 3:12:30, 0.08 it/s, Epoch 0.55/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-ff28b97e3106>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTTTrainArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhappy_tt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\happytransformer\\happy_text_to_text.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_filepath, args)\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m         \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataclass_args\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_filepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTTEvalArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\happytransformer\\tt\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_filepath, dataclass_args)\u001b[0m\n\u001b[0;32m    137\u001b[0m                 \u001b[0mdata_collator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             )\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1463\u001b[0m                         \u001b[0moptimizer_was_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscale_before\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mscale_after\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1464\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1465\u001b[1;33m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1467\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0moptimizer_was_run\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[1;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\optimization.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    357\u001b[0m                 \u001b[1;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m                 \u001b[1;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 359\u001b[1;33m                 \u001b[0mexp_avg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    360\u001b[0m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"eps\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from happytransformer import TTTrainArgs\n",
    "\n",
    "args = TTTrainArgs(batch_size=8)\n",
    "happy_tt.train(\"train.csv\", args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd8040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
